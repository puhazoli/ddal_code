{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","collapsed_sections":["WxoahgH6lzky","Osf2HOU0mECv","iyIn-umRSbLa"],"mount_file_id":"1LnT4jeU0r15OguIU8D9QdQlsvEXUwm4W","authorship_tag":"ABX9TyPY8QRCDlV1ZMRYtgj+HxwN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Installing packages\n","\n","This cell is needed to (re)install packages that are needed for the successful completion of the notebook. Currently only works with GPU backend."],"metadata":{"id":"Z3C-GbScdwoR"}},{"cell_type":"code","source":["# import os\n","# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""],"metadata":{"id":"yOqv7fCkKYjC"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9XpuffkPw8e_"},"source":["try:\n","  import asbe\n","except:\n","  !pip install --upgrade --force-reinstall git+https://github.com/puhazoli/asbe.git\n","  !pip install pytorch-lightning\n","  !pip install scikit-uplift\n","  !pip install pylift\n","  !pip install causeinfer\n","  !pip install matplotlib==3.7.1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Loading packages and functions and creating classes\n","\n","Also, we are defining here functions such as the weighted loss, or classes for DGPs and the base neural network"],"metadata":{"id":"qOBo144aeI2a"}},{"cell_type":"code","metadata":{"id":"rK9OL0YiDdz2"},"source":["from sklearn.base import BaseEstimator\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset, random_split\n","import pytorch_lightning as pl\n","\n","from typing import Union, Callable, Optional, Tuple, List, Iterator, Any\n","from copy import deepcopy\n","from dataclasses import dataclass, field\n","\n","import pandas as pd\n","from scipy.optimize import linear_sum_assignment\n","\n","\n","from asbe.base import *\n","from asbe.models import *\n","from asbe.helper import *\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import normalize\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","from sklearn.neighbors import KernelDensity\n","\n","from scipy.stats import rankdata\n","\n","import pickle\n","import io"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from psutil import Process"],"metadata":{"id":"PQaetJRBcwBI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.set_float32_matmul_precision('medium')"],"metadata":{"id":"Y5wHexZoOKs4"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QlrJcT1xHopX"},"source":["def weighted_loss(output, target, t, weights):\n","    \"\"\"Function that returns the weighted meas squared loss\"\"\"\n","    return torch.mean(weights * (output - target)**2)\n","\n","def pdist2sq(x,y):\n","    x2 = torch.sum(x ** 2, dim=1, keepdims=True)\n","    y2 = torch.sum(y ** 2, dim=1, keepdims=True)\n","    dist = x2 + torch.transpose(y2, 1, 0) - 2. * torch.matmul(x, torch.transpose(y, 1, 0))\n","    return dist"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f7dDCEn-IO1F"},"source":["class IHDP(Dataset):\n","    def __init__(self, path, subset=False):\n","        self.data = pd.read_csv(path)\n","        if subset:\n","            self.data = self.data[self.data[\"treatment\"]==0].reset_index(drop=True)\n","        self.y = self.data[\"y_factual\"].to_numpy()\n","        self.ite = np.where(self.data[\"treatment\"] == 1, self.data[\"y_factual\"] - self.data[\"y_cfactual\"], self.data[\"y_cfactual\"] - self.data[\"y_factual\"])\n","\n","    def __len__(self):\n","        return self.data.shape[0]\n","\n","    def __getitem__(self, idx):\n","        return [torch.from_numpy(self.data.iloc[idx,5:].\\\n","                                      to_numpy().reshape(1, -1)).float(),\n","               torch.tensor(self.y[idx]).float(),\n","               self.ite[idx],\n","               torch.tensor(self.data.loc[idx, \"treatment\"]).float()\n","                ]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZDOVjuzd2_LM"},"source":["class ASBEDATA(Dataset):\n","    \"\"\"Class that supplies data to the neural network in X, y, ite, t format\n","\n","    Arguments:\n","    - ds : dictionary, dataset\n","    - training : boolean, if we are in training mode\n","    - pool: boolea, if we are in pool model (if both training and pool are false, than it gives test data\"\"\"\n","    def __init__(self, ds, training = True, pool = False, normalize=False):\n","       self.ds = ds\n","       self.training = training\n","       self.pool = pool\n","       self.normalize = normalize\n","       if self.normalize:\n","          self.means = np.mean(self.ds[\"X_training\"], axis=0)\n","          self.stds = np.std(self.ds[\"X_training\"], axis=0)\n","\n","    def __len__(self):\n","        if self.training:\n","            return self.ds[\"X_training\"].shape[0]\n","        else:\n","            return self.ds[\"X_test\"].shape[0]\n","\n","    def __getitem__(self, idx):\n","        fstring = \"training\" if self.training else \"test\"\n","        if self.pool:\n","            fstring = \"pool\"\n","        if self.normalize:\n","          X = self.ds[f\"X_{fstring}\"][idx,:].astype(float)\n","          X = (X - self.means) / self.stds\n","          X = torch.tensor(X)\n","        else:\n","          X = torch.tensor(self.ds[f\"X_{fstring}\"][idx,:])\n","        out = [X.float(),\n","                torch.tensor(self.ds[f\"y_{fstring}\"][idx]).float(),\n","                torch.tensor(self.ds[f\"ite_{fstring}\"][idx]).float(),\n","                torch.tensor(self.ds[f\"t_{fstring}\"][idx]).float()]\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DDALIPM(BaseAcquisitionFunction):\n","  \"\"\"Custom class to test out ddal\n","  \"\"\"\n","\n","  def treat_v_control(self, model, dataset, method=\"dist_to_labeled\", mmd=False, selection_count=0):\n","    t = torch.Tensor(dataset[\"t_training\"])\n","    X = torch.Tensor(dataset[\"X_training\"])\n","    X_pool = torch.Tensor(dataset[\"X_pool\"])\n","    # t_random_pool_prob = torch.empty(X_pool.size(0),1).uniform_(0, 1)\n","    t_pool = torch.Tensor(dataset[\"t_pool\"])\n","    zero_treatments = t.eq(0)\n","    one_treatments = t.eq(1)\n","    zero_pool = t_pool.eq(0)\n","    one_pool = t_pool.eq(1)\n","    # Get predictions and latent space representations\n","    model_pred =  model.model(X, t)\n","    y_pred, t_pred = model_pred[0].detach(), model_pred[1].detach()\n","    y_pool_pred, t_pool_pred = model.model(X_pool, t_pool)\n","    zero_predicted_pool = t_pool_pred.le(.5).squeeze()\n","    one_predicted_pool = t_pool_pred.ge(.500001).squeeze()\n","    phi = model.model(X, t, return_phi=True).detach()\n","    phi_pool = model.model(X_pool, t_pool, return_phi=True).detach()\n","    prop_score = float(torch.sum(t)/t.size(0))\n","    distances = torch.zeros(t_pool.size())\n","    if method == \"dist_to_labeled\":\n","      if mmd:\n","        mmmd_distances = np.zeros(t_pool.size())\n","        base_loss = model.model.mmdsq_loss(phi, t, t_pred)\n","        for ix in range(phi_pool.size(0)):\n","          phi_to_add = phi_pool[ix,:].view(1, phi_pool.size(1))\n","          phi_new = torch.cat(\n","              (phi, phi_to_add), 0)\n","          dist_change = 0\n","          zero_multiplier = t_pool_pred[ix]\n","          one_multiplier = 1 - t_pool_pred[ix]\n","          for cf in [0,1]:\n","            t_updated = torch.cat((t, torch.tensor([cf])), 0)\n","            t_pred_new =  t_pool_pred[ix].view(1,1)\n","            t_pred_updated = torch.cat((t_pred,t_pred_new),0)\n","            if cf == 0:\n","              dist_change += zero_multiplier * (base_loss - model.model.mmdsq_loss(phi_new, t_updated, t_pred_updated))\n","            if cf == 1:\n","              dist_change += one_multiplier * (base_loss -  model.model.mmdsq_loss(phi_new, t_updated, t_pred_updated))\n","          mmmd_distances[ix] = dist_change\n","        return mmmd_distances\n","      else:\n","        dist_to_c = torch.mean(pdist2sq(phi_pool[zero_predicted_pool], phi[zero_treatments]), axis=1)\n","        dist_to_t = torch.mean(pdist2sq(phi_pool[one_predicted_pool], phi[one_treatments]), axis=1)\n","    if method == \"dist_to_counter\":\n","      if mmd:\n","        mmmd_distances = np.zeros(t_pool.size())\n","        base_zero_loss = model.model.mmdsq_loss(phi[zero_treatments], t[zero_treatments], t_pred[zero_treatments])\n","        base_one_loss = model.model.mmdsq_loss(phi[one_treatments], t[one_treatments], t_pred[one_treatments])\n","        for ix in range(phi_pool.size(0)):\n","          temp_pred = t_pool_pred[ix]\n","          phi_to_add = phi_pool[ix,:].view(1, phi_pool.size(1))\n","          phi_new = torch.cat(\n","              (phi, phi_to_add), 0)\n","          # if temp_pred < .5:\n","          #   cf = 1\n","          # else:\n","          #   cf = 0\n","          for cf in [0, 1]:\n","            t_updated = torch.cat((t, torch.tensor([cf])), 0)\n","            t_pred_new =  t_pool_pred[ix].view(1,1)\n","            t_pred_updated = torch.cat((t_pred,t_pred_new),0)\n","            if cf == 0:\n","              dist_change = (1 - temp_pred) * (base_zero_loss - model.model.mmdsq_loss(phi_new[t_updated.eq(cf)],\n","                                                                    t_updated[t_updated.eq(cf)],\n","                                                                    t_pred_updated[t_updated.eq(cf)]))\n","            if cf == 1:\n","              dist_change += temp_pred * (base_one_loss -  model.model.mmdsq_loss(phi_new[t_updated.eq(cf)],\n","                                                                    t_updated[t_updated.eq(cf)],\n","                                                                    t_pred_updated[t_updated.eq(cf)]))\n","          mmmd_distances[ix] = dist_change / 2\n","        return mmmd_distances\n","      else:\n","        dist_to_c = -1 * torch.mean(pdist2sq(phi_pool[zero_predicted_pool], phi[one_treatments]), axis=1)\n","        dist_to_t = -1 * torch.mean(pdist2sq(phi_pool[one_predicted_pool], phi[zero_treatments]), axis=1)\n","    if method == \"dist_to_pool\":\n","      dist_to_c = -1 * torch.mean(pdist2sq(phi_pool[zero_predicted_pool], phi_pool[zero_pool]), axis=1)\n","      dist_to_t = -1 * torch.mean(pdist2sq(phi_pool[one_predicted_pool], phi_pool[one_pool]), axis=1)\n","    if method == \"dist_to_pool_counter\":\n","      dist_to_c = -1 * torch.mean(pdist2sq(phi_pool[zero_predicted_pool], phi_pool[one_pool]), axis=1)\n","      dist_to_t = -1 * torch.mean(pdist2sq(phi_pool[one_predicted_pool], phi_pool[zero_pool]), axis=1)\n","    if method == \"dist_to_selected\":\n","      if selection_count >= 1:\n","        print(f\"Number of selections: {selection_count}\")\n","        distances = torch.mean(pdist2sq(phi_pool, phi[-selection_count:]), axis=1)\n","        return distances.detach().numpy()\n","      else:\n","        dist_to_c = torch.ones(zero_predicted_pool.sum())\n","        dist_to_t = torch.ones(one_predicted_pool.sum())\n","    distances[zero_predicted_pool] = dist_to_c\n","    distances[one_predicted_pool] = dist_to_t\n","    distances = distances.detach().numpy()\n","    # _, t_pred = model.model(X, t)\n","    # _, t_pred_pool = model.model(X_pool, t_pool)\n","    # Phic, Phit = phi[zero_treatments], phi[one_treatments]\n","    # dist = model.model.mmdsq_loss(phi, t, t_pred)\n","    # ipms = np.zeros(dataset[\"X_pool\"].shape[0])\n","    # for ix in range(dataset[\"X_pool\"].shape[0]):\n","    #   phi_to_add = phi_pool[ix,:].resize(1, phi_pool.size(1))\n","    #   phi_new = torch.cat(\n","    #       (phi, phi_to_add), 0)\n","    #   t_new = t_pool[ix].resize(1)\n","    #   t_updated = torch.cat((t, t_new), 0)\n","    #   t_pred_new =  t_pred_pool[ix].resize(1,1)\n","    #   t_pred_updated = torch.cat((t_pred,t_pred_new),0)\n","    #   dist_change = model.model.mmdsq_loss(phi_new, t_updated, t_pred_updated)\n","    #   #tempT = model.model.mmdsq_loss(phi_new, torch.cat((t, torch.Tensor([1])), 0), torch.cat((t_pred, t_pred_new),0))\n","    #   ipms[ix] = dist - dist_change\n","    return distances\n","\n","  def train_v_pool(self, model, dataset):\n","    # t = torch.Tensor(dataset[\"t_training\"])\n","    # X = torch.Tensor(dataset[\"X_training\"])\n","    X_pool = torch.Tensor(dataset[\"X_pool\"])\n","    # t_random_pool_prob = torch.empty(X_pool.size(0),1).uniform_(0, 1)\n","    # t_pool = torch.bernoulli(t_random_pool_prob)\n","    t_pool = torch.Tensor(dataset[\"t_pool\"])\n","    # zero_treatments = t.eq(0)\n","    # one_treatments = t.eq(1)\n","    # phi = model.model(X, t, return_phi=True)\n","    phi_pool = model.model(X_pool, t_pool, return_phi=True)\n","    dist = -1 * torch.mean(pdist2sq(phi_pool, phi_pool), axis=1).detach().numpy()\n","    # ipms = np.zeros(dataset[\"X_pool\"].shape[0])\n","    # num_train = dataset[\"X_training\"].shape[0]\n","    # X_all = torch.Tensor(np.concatenate([dataset[\"X_training\"], dataset[\"X_pool\"]]))\n","    # t_trainpool = torch.Tensor(np.concatenate([np.ones(dataset[\"X_training\"].shape[0]),\n","    #                                np.zeros(dataset[\"t_pool\"].shape[0])]))\n","    # zero_treatments = t_trainpool.eq(0)\n","    # one_treatments = t_trainpool.eq(1)\n","    # phi = model.model(X_all, t_trainpool, return_phi=True)\n","    # Phic, Phit = phi[zero_treatments], phi[one_treatments]\n","    # _, t_pred = model.model(X_all, t_trainpool)\n","    # dist = model.model.mmdsq_loss(phi, t_trainpool, t_pred)\n","    # for ix in range(dataset[\"X_pool\"].shape[0]):\n","    #   t_trainpool[(num_train + ix)] = 1\n","    #   temp_ipm = model.model.mmdsq_loss(phi, t_trainpool, t_pred)\n","    #   ipms[ix] += dist - temp_ipm\n","    #   t_trainpool[(num_train + ix)] = 0\n","    return dist\n","\n","  def uncertainty(self, model, dataset):\n","    pred = model.predict(X=dataset[\"X_pool\"], return_mean=False)\n","    unc =  pred.var(1).detach().numpy()\n","    return unc\n","\n","  def subsample(self, dataset, number_to_get):\n","    sub_ix = np.random.randint(0, dataset[\"X_pool\"].shape[0], number_to_get)\n","    subsampled = deepcopy(dataset)\n","    for key, value in subsampled.items():\n","      if \"pool\" in key:\n","        if key.startswith(\"X\"):\n","          subsampled[key] = subsampled[key][sub_ix, :]\n","        else:\n","          subsampled[key] = subsampled[key][sub_ix]\n","    return([sub_ix, subsampled])\n","\n","  def kde(self, model, dataset):\n","    X = torch.Tensor(np.concatenate((dataset[\"X_training\"], dataset[\"X_pool\"])))\n","    t = torch.Tensor(np.concatenate((dataset[\"t_training\"], dataset[\"t_pool\"])))\n","    phi = model.model(X, t, return_phi=True).detach().numpy()\n","    kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(phi)\n","    kde_scores = kde.score_samples(phi)[(dataset[\"X_training\"].shape[0]):]\n","    return kde_scores\n","\n","  def _normalize(self, metric):\n","    return (metric + .0001 - np.min(metric + .0001))/np.max(metric + .0001)\n","\n","  def calculate_metrics(self, model, dataset, **kwargs):\n","    scores = np.zeros(dataset[\"X_pool\"].shape[0])\n","    self.N_pool = dataset[\"X_pool\"].shape[0]\n","    self.d_data = dataset[\"X_pool\"].shape[1]\n","    if \"mode\" not in kwargs:\n","      raise ValueError(\"No mode has been supplied\")\n","    if type(kwargs[\"mode\"]) is str:\n","      mode = [kwargs[\"mode\"]]\n","    else:\n","      mode = kwargs[\"mode\"]\n","    if \"uncertainty\" in mode:\n","      scores += self._normalize(self.uncertainty(model, dataset))\n","    if \"treat_v_control\" in mode:\n","      method_to_run = kwargs[\"method\"] if \"method\" in kwargs.keys() else \"dist_to_labeled\"\n","      mmd_to_run = kwargs[\"mmd\"] if \"mmd\" in kwargs.keys() else False\n","      scores += self._normalize(self.treat_v_control(model, dataset, method=method_to_run, mmd = mmd_to_run))\n","      #ipms = ipms.detach().numpy()\n","      # scores += ipms\n","    if \"train_v_pool\" in mode:\n","      scores += self._normalize(self.train_v_pool(model, dataset))\n","      # scores += ipms / np.linalg.norm(ipms, ord=1)\n","    if \"batch\" in mode:\n","      res = list()\n","      selected = list()\n","      mmd_to_run = kwargs[\"mmd\"] if \"mmd\" in kwargs.keys() else False\n","      inb_tc = self.treat_v_control(model, dataset, \"dist_to_labeled\", mmd = mmd_to_run)\n","      #inb_dp = self.treat_v_control(model, dataset, \"dist_to_pool\", mmd = mmd_to_run)\n","      inb_dc = self.treat_v_control(model, dataset, \"dist_to_counter\", mmd = mmd_to_run)\n","      #inb_dcp = self.treat_v_control(model, dataset, \"dist_to_pool_counter\", mmd = mmd_to_run)\n","      inb_uc = self.uncertainty(model, dataset)\n","      for sim in range(kwargs[\"simulations\"]):\n","        ix = np.random.choice(dataset[\"t_pool\"].shape[0], self.no_query)\n","        temp_res = dict()\n","        temp_res[\"sim\"] = sim\n","        temp_res[\"tc\"] = np.mean(inb_tc[ix])\n","        #temp_res[\"dp\"] = np.mean(inb_dp[ix])\n","        # temp_res[\"dcp\"] = np.mean(inb_dcp[ix])\n","        temp_res[\"dc\"] = np.mean(inb_dc[ix])\n","        temp_res[\"uc\"] = np.mean(inb_uc[ix])\n","        temp_res[\"kde\"] = np.mean(kde_scores[ix])\n","        #temp_res[\"prop_score\"] = -1 * (.5 - np.mean(dataset[\"t_pool\"][ix]))\n","        res.append(temp_res)\n","        selected.append(ix)\n","      rdt = pd.DataFrame(res)\n","      #print(rankdata(temp_res[\"tc\"].ravel()))\n","      rdt[\"ranking_score\"] = rankdata(\n","          rdt[\"tc\"]) + rankdata(\n","              rdt[\"dc\"]) + rankdata(\n","                  rdt[\"uc\"] + rankdata(rdt[\"kde\"])) #+ rankdata(rdt[\"uc\"]) + rankdata(rdt[\"prop_score\"])\n","      rdt.reset_index(inplace=True)\n","      best_group = rdt.loc[rdt[\"ranking_score\"].idxmax(), \"sim\"]\n","      scores[selected[best_group]] = 1\n","    if \"accounting\" in mode:\n","      sel = []\n","      ds = deepcopy(dataset)\n","      # _, t_pred = model.model(torch.Tensor(ds[\"X_pool\"]), torch.Tensor(ds[\"t_pool\"]))\n","      # t_pred_np = t_pred.detach().numpy()\n","      ixs = np.arange(ds[\"X_pool\"].shape[0])\n","      inb_uc = self.uncertainty(model, ds)\n","      # kde_sc = self.kde(model, ds)\n","      for counter in range(self.no_query):\n","        subsampled_ix, subsampled_data = self.subsample(ds, 1000)\n","        if counter % 100 == 0:\n","          print(f\"Currently at {counter}\")\n","        # prop_score = np.sum(ds[\"t_training\"]) / ds[\"t_training\"].shape[0]\n","        # non_matching = ds[\"t_pool\"] != random_draw\n","        # random_draw = np.random.binomial(1, 0.3)\n","        # if random_draw == 1:\n","          # six = np.random.choice(ds[\"X_pool\"].shape[0], 1)[0]\n","        # else:\n","        mmd_to_run = kwargs[\"mmd\"] if \"mmd\" in kwargs.keys() else False\n","        inb_tc =  -1 * self.treat_v_control(model, subsampled_data, \"dist_to_labeled\", mmd = mmd_to_run)\n","        # inb_dp =  self.treat_v_control(model, ds, \"dist_to_pool\", mmd = mmd_to_run)\n","        inb_dc =  self.treat_v_control(model, subsampled_data, \"dist_to_counter\", mmd = mmd_to_run)\n","        inb_ds =  self.treat_v_control(model, subsampled_data, \"dist_to_selected\", mmd = mmd_to_run, selection_count = counter)\n","        # inb_dcp = self.treat_v_control(model, ds, \"dist_to_pool_counter\", mmd = mmd_to_run)\n","        r_tc = rankdata(inb_tc)\n","        #r_tp = rankdata(inb_dp)\n","        r_dc = rankdata(inb_dc)\n","        # r_dcp = rankdata(inb_dcp)\n","        r_unc = rankdata(inb_uc[subsampled_ix])\n","        # r_kde = rankdata(kde_sc)\n","        r_ds = rankdata(inb_ds)\n","\n","        sub_ix = np.argmax(#r_tc +\n","                        r_dc + r_unc + r_tc + r_ds)\n","        # six = np.argmax(inb_tc+inb_dp+inb_dc+inb_dcp + inb_uc)\n","        # six = np.argmax(r_tc + r_tp + r_dc + r_unc + r_dcp)\n","        #six = np.argmax(r_tc + r_unc + r_dc + r_tp)\n","        six = subsampled_ix[sub_ix]\n","        sel.append(ixs[six])\n","        ds[\"X_training\"] = np.concatenate((ds[\"X_training\"],\n","                                           ds[\"X_pool\"][six,:].reshape((1, -1))),\n","                                          axis=0)\n","        ds[\"t_training\"] = np.concatenate((ds[\"t_training\"], ds[\"t_pool\"][six].ravel()),\n","                                          axis=0)\n","        ds[\"X_pool\"] = np.delete(ds[\"X_pool\"], six, axis=0)\n","        ds[\"t_pool\"] = np.delete(ds[\"t_pool\"], six)\n","        ixs = np.delete(ixs, six)\n","        # kde_sc = np.delete(kde_sc, six)\n","        #t_pred_np = np.delete(t_pred_np, six)\n","        inb_uc = np.delete(inb_uc, six)\n","        # inb_uc = np.delete(inb_uc, six)\n","      scores[sel] = 1\n","    return scores"],"metadata":{"id":"XYjjefYQJWj2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LightTarNetIPM(pl.LightningModule):\n","    \"\"\"Lightweight PL implementation of the TARnet architecture\n","\n","    Arguments:\n","    - weights : weights according to the calculation in the original paper, based on number of control and treated units\n","    - num_features : Number of features, so neural network can be instantianeted \"\"\"\n","    def __init__(self, alpha, sigma, pt, weights, num_features, conv, binary):\n","        super().__init__()\n","        print(\"Initializing model....\")\n","        self.binary = binary\n","        self.conv = conv\n","        self.alpha = alpha\n","        self.rbf_sigma = sigma\n","        self.pt = pt\n","        if conv:\n","          self.conv1 = nn.Conv2d(1,\n","                                  32,\n","                                  kernel_size=7,\n","                                  stride=(1,1),\n","                                  padding=\"same\")\n","          self.conv2 = nn.Conv2d(32,\n","                                  64,\n","                                  kernel_size=5,\n","                                  stride=(1,1),\n","                                  padding=\"same\")\n","        fc_features = 64 * 28 * 28 if self.conv else num_features\n","        self.fc1 = nn.Linear(fc_features, 200)\n","        self.fc2 = nn.Linear(200, 200)\n","        self.fc3 = nn.Linear(200, 25)\n","        self.t_hidden = nn.Linear(25, 25)\n","        self.t_pred = nn.Linear(25, 1)\n","        #self.fc3 = nn.Linear(200, 200)\n","        # T == 1\n","        self.t1fc1 = nn.Linear(25, 200)\n","        self.t1fc2 = nn.Linear(200, 200)\n","        self.t1fc3 = nn.Linear(200, 1)\n","        # T == 0\n","        self.t0fc1 = nn.Linear(25, 200)\n","        self.t0fc2 = nn.Linear(200, 200)\n","        self.t0fc3 = nn.Linear(200, 1)\n","        self.drop_layer = nn.Dropout(p=0.1)\n","        self.batch_norm = nn.BatchNorm1d(25)\n","        self.weights = weights\n","\n","    def K(self, xi, xj, sigma):\n","        return torch.exp( -0.5* sigma**2 * torch.sum((xi - xj)**2))\n","\n","    def RBF_K(self, xi, xj):\n","        return torch.exp(-pdist2sq(xi,xj)/self.rbf_sigma**2)\n","\n","    def mmdsq(self, Phi, t, t_pred):\n","        zero_treatments = t.eq(0)\n","        one_treatments = t.eq(1)\n","        Phic, Phit = Phi[zero_treatments], Phi[one_treatments]\n","        tpredc, tpredt = t_pred[zero_treatments], t_pred[one_treatments]\n","        weightc = torch.Tensor((1-self.pt)/(1.0001-tpredc)).view(-1,1)\n","        weightt = torch.Tensor(self.pt/(tpredt+0.0001)).view(-1,1)\n","        m = Phic.size(0)\n","        n = Phit.size(0)\n","        if m > 0:\n","          weightedphic = weightc * Phic\n","          Kcc = self.RBF_K(weightedphic,weightedphic)\n","          if m <= 1:\n","            mmd = 0\n","          else:\n","            mmd = 1.0/(m*(m-1.0))*(torch.sum(Kcc))\n","        else:\n","          mmd = 0\n","        if n > 0:\n","          weightedphit = weightt * Phit\n","          Ktt = self.RBF_K(weightedphit,weightedphit)\n","          if n <= 1:\n","            mmd += 0\n","          else:\n","            mmd = mmd + 1.0/(n*(n-1.0))*(torch.sum(Ktt))\n","        else:\n","          mmd += 0\n","        if m > 0 and n > 0:\n","          Kct = self.RBF_K(weightedphic,weightedphit)\n","          mmd = mmd - 2.0/(m*n)*torch.sum(Kct)\n","        else:\n","          mmd += 0\n","        return mmd * torch.ones_like(t)\n","\n","    def mmdsq_loss(self, phi, t, t_pred):\n","        mmdsq_loss = torch.mean(self.mmdsq(phi, t, t_pred))\n","        return mmdsq_loss\n","\n","    def forward(self, x, t, return_phi=False):\n","        if self.conv:\n","          x = F.elu(self.conv1(x.view(x.size(0), 1, 28, 28)))\n","          x = F.elu(self.conv2(x))\n","        x = x.view(x.size(0), -1)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = F.relu(self.fc3(x))\n","\n","        t_latent = F.relu(self.t_hidden(x))\n","        t_hat = (torch.sigmoid(self.t_pred(t_latent))+ 0.001) / 1.002\n","        #x = F.elu(self.fc3(x))\n","        if return_phi:\n","          return x\n","        zero_treatments = t.eq(0)\n","        one_treatments = t.eq(1)\n","        x0 = torch.clone(x)\n","        x1 = torch.clone(x)\n","        x0 = x0.type_as(x)\n","        x1 = x1.type_as(x)\n","\n","        x0[~zero_treatments, :] = 0\n","        x1[zero_treatments,: ] = 0\n","        x0 = F.relu(self.t0fc1(x0))\n","        x0 = F.relu(self.t0fc2(x0))\n","        if self.binary:\n","          x0 = F.sigmoid(self.t0fc3(x0))\n","          x0 = self.drop_layer(x0)\n","        else:\n","          x0 = self.t0fc3(x0)\n","          x0 = self.drop_layer(x0)\n","\n","        x1 = F.relu(self.t1fc1(x1))\n","        x1 = F.relu(self.t1fc2(x1))\n","        if self.binary:\n","          x1 = F.sigmoid(self.t1fc3(x1))\n","          x1 = self.drop_layer(x1)\n","        else:\n","          x1 = self.t1fc3(x1)\n","          x1 = self.drop_layer(x1)\n","        out = torch.cat((x0,x1), 1)\n","        out = torch.gather(out, 1, t.long().unsqueeze(-1))\n","        # if self.conv or self.binary:\n","        #   out = nn.bernoulli(out)\n","        return (out, t_hat)\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)\n","        return optimizer\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y, ite, t = batch\n","        y_hat, t_hat = self(x, t)\n","        zero_treatments = t.eq(0)\n","        one_treatments = t.eq(1)\n","        #tpredc, tpredt = t_hat[zero_treatments], t_hat[one_treatments]\n","        weightc = (1-self.pt)/(1.0001-t_hat)\n","        weightt = self.pt/(t_hat+0.0001)\n","\n","        phi = self(x, t, return_phi=True)\n","        mmd_loss = self.mmdsq_loss(phi, t, t_hat)\n","        if self.conv or self.binary:\n","          loss = F.mse_loss(y_hat, y) + self.alpha * mmd_loss\n","          #return loss0\n","        else:\n","          loss0 = torch.mean((1 - t) * torch.square(y - y_hat)*weightc)\n","          loss1 = torch.mean(t * torch.square(y - y_hat)* weightt)\n","          loss =  loss0 + loss1 + self.alpha * mmd_loss\n","        return loss\n","        #loss = F.mse_loss(y_hat, y.view(y.size(0), -1))\n","        #self.log(\"train_loss\", loss)\n","\n","\n","    def test_step(self, batch, batch_idx):\n","        x, y, ite, t = batch\n","        t0 = torch.zeros_like(t)\n","        t1 = torch.ones_like(t)\n","        y_hat0 = self(x, t0)[0]\n","        y_hat1 = self(x, t1)[0]\n","        ite_hat = y_hat1 - y_hat0\n","        if self.conv:\n","          loss = F.mse_loss(ite_hat, ite.view(ite.size(0), -1))\n","        else:\n","          loss = F.mse_loss(ite_hat, ite.view(ite.size(0), -1))\n","        self.log('test_loss', loss)"],"metadata":{"id":"wHEHhbHnJ-KE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ASBETARNETIPM(BaseITEEstimator):\n","  \"\"\"Class that wraps around the TARnet to work with asbe\n","\n","  Arguments:\n","  - two_model : placeholder, currently has no function\n","  - num_sim : number of passes through the NN when predicting\n","  - tpu: if TPU or GPU is used - current version only works with GPU\n","  \"\"\"\n","  def __init__(self, two_model, num_sim = 100, conv = False, tpu=False, epochs=300, binary_outcome=False):\n","      super().__init__()\n","      self.binary = binary_outcome\n","      self.conv = conv\n","      self.num_sim = num_sim\n","      self.tpu = tpu\n","      self.num_epochs = epochs\n","\n","  def prepare_data(self, X_training=None, t_training=None, y_training=None, ps_scores=None):\n","    return X_training, t_training, y_training, ps_scores\n","\n","  def fit(self, **kwargs):\n","      # num_epochs = self.num_epochs if \"epochs\" in self.__dict__ else 700\n","      u = (1/ kwargs[\"t_training\"].shape[0])*kwargs[\"t_training\"].sum()\n","      ws = torch.from_numpy((kwargs[\"t_training\"]/(2*u) + (\n","                           1-kwargs[\"t_training\"])/(2*(1-u))))\n","      pt = np.sum(kwargs[\"t_training\"] == 1) / kwargs[\"X_training\"].shape[0]\n","      self.model = LightTarNetIPM(alpha=1, sigma=1,\n","                               weights=ws,\n","                               pt = pt,\n","                               conv=self.conv,\n","                               binary=self.binary,\n","                               num_features=kwargs[\"X_training\"].shape[1])\n","      if self.tpu:\n","          self.trainer = pl.Trainer(max_epochs=600, tpu_cores=8)\n","      else:\n","          self.trainer = pl.Trainer(max_epochs=self.num_epochs,\n","                                    accelerator=\"gpu\",\n","                                    enable_progress_bar=True,\n","                                    enable_model_summary=False)\n","          # self.trainer.tune(self.model)\n","      data = ASBEDATA(ds = self.dataset)\n","      dl = DataLoader(data, batch_size = 50, num_workers=4)\n","      self.trainer.fit(self.model, dl)\n","\n","  def predict(self, **kwargs):\n","      ret_mean = True if \"return_mean\" not in kwargs else kwargs[\"return_mean\"]\n","      pbool = False if \"pool\" not in kwargs else kwargs[\"pool\"]\n","      #tr = kwargs[\"training\"] if \"training\" in kwargs else True\n","      if \"X\" not in kwargs:\n","        preddata = ASBEDATA(self.dataset, training=False, pool=pbool)\n","        preddata = preddata[:][0]\n","      else:\n","        preddata = torch.tensor(kwargs[\"X\"]).float()\n","      #dl = DataLoader(preddata, batch_size = 1000, num_workers=1)\n","      n_pred = preddata.shape[0]\n","      if ret_mean:\n","          out1 = self.model(preddata, torch.ones(n_pred))[0]\n","          out0 = self.model(preddata, torch.zeros(n_pred))[0]\n","          ret = (out1-out0).detach().numpy()\n","      else:\n","          #self.model = self.model.train()\n","          out1 = torch.cat([self.model(preddata, torch.ones(n_pred))[0].detach() for _ in range(self.num_sim)], dim=1)\n","          out0 = torch.cat([self.model(preddata, torch.zeros(n_pred))[0].detach() for _ in range(self.num_sim)], dim=1)\n","          ret = out1 - out0\n","      return ret"],"metadata":{"id":"BjC7ABn_JSoz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class IPAssignment(BaseAssignmentFunction):\n","  def select_treatment(self, model, dataset, query_idx):\n","    y_pred, t_pred = model.model(torch.Tensor(dataset[\"X_pool\"][query_idx, :]),\n","                                 torch.Tensor(dataset[\"t_pool\"][query_idx]))\n","    ex = 1 - t_pred.detach().numpy()\n","    out = np.random.binomial(1, ex).squeeze()\n","    return out"],"metadata":{"id":"aMKxW_o9cJey"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Running test code for tarnet\n","\n","In this section we load an IHDP dataset and try out the model with the above defined classes\n","\n"],"metadata":{"id":"WxoahgH6lzky"}},{"cell_type":"code","source":["def quick_sim(N1, N2, seed=1, ihdp=1, run_random=True):\n","  score = dict()\n","  N1_perc = (747-N1)/747\n","  ds = get_ihdp_dict(ihdp, N1_perc, True, 0.1, seed=seed)\n","  print(f'Propensity before: {np.sum(ds[\"t_training\"]) /ds[\"t_training\"].shape[0]}')\n","  copied_ds = deepcopy(ds)\n","  pt = np.sum(copied_ds[\"t_training\"])/copied_ds[\"t_training\"].shape[0]\n","  # Pool size is 604, Train size is 74\n","  # try:\n","  if run_random:\n","    asl = BaseActiveLearner(estimator = ASBETARNETIPM(two_model=False),\n","                          acquisition_function=RandomAcquisitionFunction(no_query=N2),\n","                          assignment_function=BaseAssignmentFunction(),\n","                          stopping_function = None,\n","                          dataset=ds)\n","    asl.estimator.dataset = asl.dataset\n","    _, random_sel = asl.query(no_query=N2)\n","    asl.teach(random_sel)\n","    asl.fit()\n","    score[\"random\"] = asl.score()\n","    print(f'Propensity after: {np.sum(asl.dataset[\"t_training\"]) /asl.dataset[\"t_training\"].shape[0]}')\n","  # DDAL\n","  # asl.dataset = copied_ds\n","  asl_ddal = BaseActiveLearner(estimator = ASBETARNETIPM(two_model=False),\n","                        acquisition_function=DDALIPM(no_query=N2),\n","                        assignment_function=IPAssignment(),\n","                        stopping_function = None,\n","                        dataset=ds)\n","  asl_ddal.estimator.dataset = asl_ddal.dataset\n","  asl_ddal.fit()\n","  # _, acc_sel = asl_ddal.query(no_query=N2, mode=\"batch\", mmd=False, simulations=100)\n","  _, mmd_sel = asl_ddal.query(no_query=N2, mode=\"accounting\",mmd=False)\n","  # _, acc_sel_mmd = asl_ddal.query(no_query=N2, mode=\"batch\", mmd=True, simulations=100)\n","  _, mmd_sel_mmd = asl_ddal.query(no_query=N2, mode=\"accounting\",mmd=True)\n","\n","  _, unc_sel = asl_ddal.query(no_query=N2, mode=\"uncertainty\")\n","  asl_ddal.teach(unc_sel)\n","  asl_ddal.fit()\n","  score[\"uncertainty\"] = asl_ddal.score()\n","\n","\n","  # DDAL MMD\n","  asl_ddal.dataset = deepcopy(copied_ds)\n","  asl_ddal.estimator.dataset = deepcopy(copied_ds)\n","  asl_ddal.teach(mmd_sel)\n","  asl_ddal.fit()\n","  score[\"accounting\"] = asl_ddal.score()\n","\n","  asl_ddal.dataset = deepcopy(copied_ds)\n","  asl_ddal.estimator.dataset = deepcopy(copied_ds)\n","  asl_ddal.teach(mmd_sel_mmd)\n","  asl_ddal.fit()\n","  score[\"accounting_mmd\"] = asl_ddal.score()\n","  # except:\n","  #   score = {\"random\":np.nan,\n","  #            \"batch\":np.nan,\n","  #            \"batch_mmd\":np.nan,\n","  #            \"accounting\":np.nan,\n","  #            \"accounting_mmd\":np.nan}\n","  print(score)\n","  return(score)"],"metadata":{"id":"n9Hct_3QP1Ah"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rHd58kGR2ZyI"},"source":["# # random: 4.62, dropout at 0.1 = 4.16\n","# res = dict()\n","# for ihdp in [10]:\n","#   res[ihdp] = dict()\n","#   for i in range(10):\n","#     res[ihdp][i] = quick_sim(50, 50, ihdp=ihdp, seed=i,run_random=True)\n","  # ch = quick_sim(50, 50, ihdp=1, seed=i,run_random=False)\n","# res_all = list()\n","# for N1 in [50, 100]:\n","#   for N2 in [25, 50, 100]:\n","#     for ihdp in range(9):\n","#       for seed in range(10):\n","#         sc = quick_sim(N1, N2, ihdp=ihdp+1, seed=seed)\n","#         sc[\"ihdp\"] = ihdp+1\n","#         sc[\"seed\"] = seed\n","#         sc[\"N1\"] = N1\n","#         sc[\"N2\"] = N2\n","#         res_all.append(sc)\n","#         print(f\"N1: {N1}, N2: {N2}, Ihdp: {ihdp}, seed: {seed}, values: {sc}\")\n","#     pd.DataFrame(res_all).to_csv(\"/drive/MyDrive/Colab Notebooks/data/ihdp_quick_sim.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","pd.json_normalize([{'random': 13.261134976726014, 'uncertainty': 12.654133973623953, 'accounting': 13.576951213858909, 'accounting_mmd': 14.117787994069714},\n","{'random': 14.85508058355626, 'uncertainty': 14.286718574039888, 'accounting': 14.387758800415256, 'accounting_mmd': 14.835449686755718},\n","{'random': 13.754437282442838, 'uncertainty': 13.344558513854864, 'accounting': 12.636939894258756, 'accounting_mmd': 13.34692728040138},\n","{'random': 13.214758048534557, 'uncertainty': 13.338884998142, 'accounting': 14.036059370054994, 'accounting_mmd': 14.37753368447979},\n","{'random': 13.889522192485034, 'uncertainty': 14.38458876320927, 'accounting': 12.584737184837856, 'accounting_mmd': 12.959747216785916},\n","{'random': 12.223091326849154, 'uncertainty': 14.211100248717251, 'accounting': 14.02388522960542, 'accounting_mmd': 13.057153276908823},\n","{'random': 14.010992729615008, 'uncertainty': 16.488582613143414, 'accounting': 14.615684341811795, 'accounting_mmd': 14.36954815656109},\n","{'random': 16.08462250117471, 'uncertainty': 16.92655346649779, 'accounting': 15.999569473701493, 'accounting_mmd': 16.104217618220588},\n","{'random': 13.318922183767329, 'uncertainty': 13.535266352679773, 'accounting': 11.379797683443073, 'accounting_mmd': 13.696285944397179},\n","{'random': 14.272557554608497, 'uncertainty': 13.726303219788115, 'accounting': 13.497567905528156, 'accounting_mmd': 11.331218013014523}]).mean()"],"metadata":{"id":"maL8ayZwaGml"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_unc = pd.json_normalize(uncertainty_d).T.reset_index()"],"metadata":{"id":"G7x8uFEjvx7I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_unc[[\"ihdp\", \"split\", \"method\"]] = df_unc[\"index\"].str.split(\".\", expand=True)\n","df_unc.rename(columns={0:\"pehe\"}, inplace=True)"],"metadata":{"id":"kOjniiUYMZ0K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.concat([df, df_unc])"],"metadata":{"id":"HiniObVlC-94"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.to_csv(\"drive/MyDrive/Colab Notebooks/data/ihdp_10_simulations_3_to_6.csv\")"],"metadata":{"id":"aNxathut0nXE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.groupby([\"ihdp\", \"method\"])[\"pehe\"].mean().plot(kind=\"bar\")"],"metadata":{"id":"zq-AWyng1wgM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"id":"ywDV2neFFftN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from google.colab import runtime\n","# runtime.unassign()"],"metadata":{"id":"KcEc3rzgVCn8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df = pd.DataFrame(res_all, columns=[\"random\", \"ddal\",\"ihdp\",\"seed\"])\n","# df.groupby([\"ihdp\"])[\"random\", \"ddal\"].mean()"],"metadata":{"id":"07NAdAC7eAYf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Simulating easy DGP\n","The aim here is to give a high-level overview with different acquisition function and see which they select, we turn off parts of the acquisition function proposed to see their effect and gain some insights"],"metadata":{"id":"e6HVmHfPoH4d"}},{"cell_type":"code","source":["X  = np.linspace(-4, 4, num = 1000)\n","y0 = np.abs(X) * 3\n","ite = (X**3 - 5)/10\n","y1 = y0 + ite\n","t = np.random.binomial(1, p=.5, size=1000)\n","y = np.where(t == 1, y1, y0) + np.random.normal(0, 0.01, size=1000)"],"metadata":{"id":"fG5clUwsobLu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(X, ite, \"b+\")\n","plt.title(\"ITE function\");"],"metadata":{"id":"_H8_qCKXszcK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prob0 = np.array([0.8 if x < 0 else 0.2 for x in X])\n","prob1 = np.array([0.8 if x > 0 else 0.2 for x in X])\n","selected0 = np.random.choice(np.arange(X.shape[0]), size = 3, p = prob0 / np.sum(prob0))\n","prob1[selected0] = 0\n","selected1 = np.random.choice(np.arange(X.shape[0]), size = 7, p = prob1 / np.sum(prob1))\n","dat0 = X[selected0]\n","dat1 = X[selected1]\n","X_train = np.concatenate((dat0, dat1), axis=0)\n","t_train = np.concatenate((np.zeros(dat0.shape[0]), np.ones(dat1.shape[0])))\n","y_train = np.concatenate((y0[selected0], y1[selected1]))"],"metadata":{"id":"s3pxNIOS5CU9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(X_train[:3], y_train[:3], \"b+\")\n","plt.plot(X_train[3:], y_train[3:], \"r*\")\n","plt.title(\"Training data\");"],"metadata":{"id":"89PG6q3gZPaW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["simds = {\"X_training\" : X_train.reshape((-1, 1)),\n","         \"y_training\" : y_train,\n","         \"t_training\" : t_train,\n","        \"ite_training\" : ite[np.concatenate([selected0, selected1])],\n","         \"X_pool\" : np.delete(X, np.concatenate([selected0, selected1])).reshape((-1, 1)),\n","         \"t_pool\" : np.delete(t, np.concatenate([selected0, selected1])),\n","         \"y_pool\" : np.delete(y, np.concatenate([selected0, selected1])),\n","         \"ite_pool\": np.delete(ite, np.concatenate([selected0, selected1]))}"],"metadata":{"id":"PmJePQcXdrMF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NO_QUERY = 10\n","asl = BaseActiveLearner(estimator = ASBETARNETIPM(two_model=False, epochs=50),\n","                        acquisition_function=DDALIPM(no_query=NO_QUERY),\n","                        assignment_function=IPAssignment(),\n","                        stopping_function = None,\n","                        dataset=simds,\n","                        no_query=NO_QUERY)\n","asl.estimator.dataset = asl.dataset\n","asl.fit()"],"metadata":{"id":"4jpq-1N9hafO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ranking_dist_to_l = rankdata(asl.acquisition_function.treat_v_control(asl.estimator, asl.dataset,\n","                                                                      \"dist_to_labeled\", mmd = False))\n","plt.plot(X_train, np.zeros(10) - 1, \"b+\")\n","plt.plot(asl.dataset[\"X_pool\"], ranking_dist_to_l)\n","plt.title(\"Ranking of distance to labeled units measurement\")\n","plt.xlabel(\"X (crosses represent the 10 training data points)\")\n","plt.ylabel(\"Ranking score\")\n","plt.savefig(\"dist_to_labeled.pdf\")\n","plt.show()"],"metadata":{"id":"QxS9TICJe3Km"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["kde_scores = asl.acquisition_function.kde(asl.estimator, asl.dataset)\n","ranking = (\n","    rankdata(asl.acquisition_function.treat_v_control(asl.estimator, asl.dataset, \"dist_to_counter\", mmd = False)) +\n","    rankdata(asl.acquisition_function.treat_v_control(asl.estimator, asl.dataset, \"dist_to_selected\", mmd = False)) +\n","    rankdata(kde_scores) +\n","    rankdata(asl.acquisition_function.uncertainty(asl.estimator, asl.dataset))\n",")\n","# sns.lineplot(x=asl.dataset[\"X_pool\"].ravel(),y=ranking).set(\n","# title =\"Ranking of units by DDAL after training on initial sample,\\nhighest score is selected for query\",\n","# xlabel =\"X for pool units\",\n","# ylabel =\"Ranking score\")\n","\n","# plt.savefig(\"ranking.pdf\")\n","# plt.show()\n","\n","# plt.plot(asl.dataset[\"X_training\"], asl.dataset[\"y_training\"], \"*\")\n","# plt.show()\n","selection = np.array([np.argmax(ranking)])\n","asl.teach(selection)\n","ranking_new = (\n","    rankdata(asl.acquisition_function.treat_v_control(asl.estimator, asl.dataset,\n","                                                      \"dist_to_selected\",\n","                                                      mmd = False,selection_count=1))\n",")\n","sns.lineplot(x=asl.dataset[\"X_pool\"].ravel(),y=ranking_new)"],"metadata":{"id":"BUwnSPum8oEU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"T8g3qoPLk_V-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ranking_new"],"metadata":{"id":"HnAuO4pDiI7g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_, unc_selection = asl.query(mode=\"uncertainty\", no_query=NO_QUERY)\n","_, tc_selection  = asl.query(mode=\"treat_v_control\", no_query=NO_QUERY)\n","_, tp_selection = asl.query(mode=\"train_v_pool\", no_query=NO_QUERY)\n","_, all_selection = asl.query(mode=[\"uncertainty\", \"treat_v_control\", \"train_v_pool\"], no_query=NO_QUERY)\n","# _, changing_selection = asl.query(mode=[\"batch\"],mmd=True,simulations=100, no_query=NO_QUERY)\n","_, rank_selection = asl.query(mode=[\"accounting\"],mmd=True, no_query=NO_QUERY)"],"metadata":{"id":"vSuIAADaT4ZR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(X_train[:3], y_train[:3], \"b+\")\n","plt.plot(X_train[3:], y_train[3:], \"r*\")\n","# #plt.plot(simds[\"X_pool\"][unc_selection, :], simds[\"y_pool\"][unc_selection], \"g*\")\n","# plt.title(\"Training data and selected units\");\n","\n","# plt.scatter(simds[\"X_pool\"][tc_selection, :], simds[\"y_pool\"][tc_selection], color=\"purple\", marker=\">\")\n","# plt.scatter(simds[\"X_pool\"][tp_selection, :], simds[\"y_pool\"][tp_selection], color=\"gray\", marker=\".\")\n","# plt.scatter(simds[\"X_pool\"][changing_selection, :], simds[\"y_pool\"][changing_selection], color=\"k\", marker=\"o\")\n","# plt.scatter(simds[\"X_pool\"][rank_selection, :], np.zeros(rank_selection.shape[0]), color=\"k\", marker=\"*\");\n","# # cycle through"],"metadata":{"id":"myw0GuoFp8Yd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pool_mean_pred = asl.estimator.predict(X=simds[\"X_pool\"], pool=True).ravel()\n","pool_mean_pred_sd = asl.estimator.predict(X=simds[\"X_pool\"], pool=True, return_mean=False).std(1).detach().numpy()"],"metadata":{"id":"UJdIXP6xlsOY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Plot for selection mechanisms\n","fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, sharex=True, sharey=False)\n","ax1.plot(X_train[:3], y_train[:3], \"bo\");\n","ax1.plot(X_train[3:], y_train[3:], \"b+\");\n","ax1.plot(X, ite, color=\"gray\", alpha=0.3);\n","ax1.set_title(\"Training data\");\n","ax2.scatter(simds[\"X_pool\"][unc_selection, :], np.arange(NO_QUERY), color=\"purple\", marker=\">\");\n","ax2.scatter(simds[\"X_pool\"].ravel(), pool_mean_pred_sd, color=\"black\");\n","ax2.set_title(\"Uncertainty selection (black points are the predicted uncertainties)\");\n","ax3.scatter(simds[\"X_pool\"][tc_selection, :], np.arange(NO_QUERY), color=\"purple\", marker=\">\");\n","ax3.scatter(simds[\"X_pool\"][tp_selection, :], np.arange(NO_QUERY), color=\"green\", marker=\"<\");\n","ax3.set_title(\"Treated vs. control selection (purple) and train vs. pool (green)\");\n","# ax4.scatter(simds[\"X_pool\"][changing_selection, :], np.arange(NO_QUERY), color=\"red\", marker=\"*\");\n","# ax4.set_title(\"Changing\");\n","ax4.scatter(simds[\"X_pool\"][rank_selection, :], np.arange(NO_QUERY), color=\"k\", marker=\"*\");\n","ax4.set_title(\"Bathch-aware DDAL (y-axis is only for better visualization)\");\n","plt.tight_layout();\n","fig.suptitle('Different acquisition strategies (gray is default ITE function)');\n","fig.subplots_adjust(top=0.88);\n","# plt.savefig(\"acq_new.pdf\")"],"metadata":{"id":"rJPTS5-bkmlI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns"],"metadata":{"id":"PYO9BphCZXtO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.histplot(pd.DataFrame({\"Uncertainty sampling\" : simds[\"X_pool\"].ravel()[unc_selection].ravel(),\n","              \"Treatment vs control sampling\": simds[\"X_pool\"][tc_selection, :].ravel(),\n","              \"Training vs pool samples\": simds[\"X_pool\"][tp_selection, :].ravel(),\n","                          \"Batch-aware DDAL sampling\": simds[\"X_pool\"][rank_selection, :].ravel()\n","              }),multiple=\"layer\", shrink=.8\n",").set(xlim=(-4, 4), title='Selection of pool units of different acquisition functions',\n","      xlabel=\"X\", ylabel=\"Number of selections\");\n","plt.savefig(\"histograms.pdf\")"],"metadata":{"id":"9y7Juyxsbqbd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = [1.5]*7 + [2.5]*2 + [3.5]*8 + [4.5]*3 + [5.5]*1 + [6.5]*8\n","sns.set_style('whitegrid')\n","sns.kdeplot(np.array(data), bw=0.5)"],"metadata":{"id":"Sz9qxTyuaYO-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Simulation study"],"metadata":{"id":"Osf2HOU0mECv"}},{"cell_type":"code","metadata":{"id":"hgKfEQuD_ia-"},"source":["SIMULATION = [0, 1, 2, 3]\n","num_query = 50\n","res_ihdp = {}\n","sim_settings = {0:  \"dist_to_labeled\",\n","                1 : \"dist_to_counter\",\n","                2:  \"dist_to_pool\",\n","                3:  \"uncertainty\"}\n","if (len(SIMULATION)) == 1:\n","    fstr = sim_settings[SIMULATION]\n","else:\n","    fstr = \"_\".join([sim_settings[x] for x in SIMULATION])\n","save_loc = f\"drive/MyDrive/Colab Notebooks/data/no_query_{fstr}_{num_query}_fixed_assignment.csv\"\n","for ihdp in range(10):\n","    ds = get_ihdp_dict(ihdp+1, 0.9, True, 0.1)\n","    asl = BaseActiveLearner(estimator = ASBETARNETIPM(two_model=False),\n","                            acquisition_function=RandomAcquisitionFunction(),\n","                            assignment_function=IPAssignment(),\n","                            stopping_function = None,\n","                            dataset=ds)\n","    asl.estimator.dataset = asl.dataset\n","    asl.fit()\n","    asl.estimator.trainer.save_checkpoint(\"first_fit.ckpt\")\n","    data_to_restore = deepcopy(asl.dataset)\n","    res = {}\n","    for i in range(10):\n","        U = (1/ asl.dataset[\"t_training\"].shape[0])*asl.dataset[\"t_training\"].sum()\n","        ws = torch.from_numpy((data_to_restore[\"t_training\"]/(2*U) + (\n","                              1-data_to_restore[\"t_training\"])/(2*(1-U))))\n","        pt = np.sum(asl.dataset[\"t_training\"] == 1) / asl.dataset[\"X_training\"].shape[0]\n","        asl.estimator.model.load_from_checkpoint(alpha =1,\n","                                                 sigma=1,\n","                                                 pt=pt,\n","                                                 weights = ws,\n","                                                 num_features = data_to_restore[\"X_training\"].shape[1],\n","                                                 checkpoint_path=\"first_fit.ckpt\")\n","        # Make query\n","        tt = torch.Tensor(asl.dataset[\"t_training\"])\n","        phi = asl.estimator.model(torch.Tensor(asl.dataset[\"X_training\"]),\n","                                 tt, return_phi = True)\n","        X_new, ix = asl.query(no_query=num_query)\n","        sc_old = asl.score()\n","        ddal = DDALIPM()\n","        if 0 in SIMULATION:\n","          # inb_dcp = self.treat_v_control(model, ds, \"dist_to_pool_counter\", mmd = mmd_to_run)\n","          avg_labeled_dist = np.mean(ddal.treat_v_control(asl.estimator,\n","                                                          data_to_restore,\n","                                                          \"dist_to_labeled\", mmd = False))\n","        if 1 in SIMULATION:\n","          avg_counter_dist = np.mean(ddal.treat_v_control(asl.estimator,\n","                                                          data_to_restore,\n","                                                          \"dist_to_counter\", mmd = False))\n","        if 2 in SIMULATION:\n","          avg_pool_dist = np.mean(ddal.treat_v_control(asl.estimator,\n","                                                       data_to_restore,\n","                                                       \"dist_to_pool\", mmd = False))\n","        if 3 in SIMULATION:\n","            X_to_pred = asl.estimator.dataset[\"X_pool\"][ix,:]\n","            pred = asl.estimator.predict(X=X_to_pred,return_mean=False)\n","            avg_uncertainty = torch.mean(pred.var(1)).detach().numpy()\n","        asl.teach(ix)\n","        asl.fit()\n","        sc_new = asl.score()\n","        res[i] = {\"dist_to_labeled\": avg_labeled_dist if 0 in SIMULATION else None,\n","                  \"dist_to_counter\": avg_counter_dist if 1 in SIMULATION else None,\n","                  \"dist_to_pool\": avg_pool_dist if 2 in SIMULATION else None,\n","                  \"uncertainty\": avg_uncertainty if 3 in SIMULATION else None,\n","                  \"pehe_change\" : sc_old - sc_new}\n","        # Restore model and data\n","        asl.dataset = deepcopy(data_to_restore)\n","        asl.estimator.dataset = deepcopy(data_to_restore)\n","        df = pd.DataFrame.from_dict(res).T\n","        #df.rename({0:\"mmd\", 1:\"loss_change\"}, axis=1, inplace=True)\n","        df[\"ihdp\"] = ihdp\n","        res_ihdp[ihdp] = df\n","        r = pd.concat(res_ihdp).reset_index(drop=True)\n","        r.to_csv(save_loc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import runtime\n","runtime.unassign()"],"metadata":{"id":"szHP3dOKzrgn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["r[~r.pehe_change.isnull()]"],"metadata":{"id":"bNZZFtVahcpv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Simulation study overlap\n","\n","First, we are concentratin on two different problems:\n","\n","The first is when the overlap assumption does not hold. Then, we need to obtain data in a way that tries to attain full overlap as soon as possible. We can test this by measuring the overlap ratio (overlap in training data, compared to overlap in the full data) and how different acquisition functions do this.\n","The second is when the data generating process is not known and the researcher might make wrong assumptions. Some acquisition functions might rely on the linearity of the treatment effect and can be suspectible to problems.\n","To test this, we run similar simulations than in the previous notebook. We use the same setup and try to understand the relationship between the overlap, DGP mismatch and decrease in PEHE.\n","\n","We use simulated data for better understanding.\n","For the overlap data, we simulate different levels of non-overlap.\n","For the covariate shift part for simulated data, we follow the Uncertainty under covariate shift part of Jesson for IHDP data."],"metadata":{"id":"iyIn-umRSbLa"}},{"cell_type":"code","source":["# Make CEMNIST data\n","def transform_image(im):\n","   return im.view((1, -1)) / 255\n","train = pd.read_csv(\"./sample_data/mnist_train_small.csv\", header=None)\n","test = pd.read_csv(\"./sample_data/mnist_test.csv\")\n","X_train = train.iloc[:, 1:].to_numpy()\n","y_train_original = train.iloc[:, 0].to_numpy()\n","X_test = test.iloc[:, 1:].to_numpy()\n","y_test_original  = test.iloc[:, 0].to_numpy()\n","\n","px_train = np.where(y_train_original == 9, 0.5, 0.5/9)\n","px_test = np.where(y_test_original == 9, 0.5, 0.5/9)\n","\n","ps_train = np.where(y_train_original == 0, 1/9, 0.5)\n","ps_test = np.where(y_test_original == 0, 1/9, 0.5)\n","\n","ps_train[y_train_original == 2] = 1\n","ps_test[y_test_original == 2] = 1\n","\n","\n","t_train = np.random.binomial(1, ps_train)\n","t_test = np.random.binomial(1, ps_test)\n","\n","y_train = np.where((\n","    (y_train_original % 2 == 1) & (t_train == 0)) |(\n","     (y_train_original % 2 == 0) & (t_train == 1)), 1, 0)\n","\n","y_test = np.where((\n","   ( y_test_original % 2 == 1) & (t_test == 0)) |(\n","         (y_test_original % 2 == 0) & (t_test == 1)), 1, 0)\n","\n","ite = np.where(y_train_original % 2 == 1, -1, 1)\n","ite_test = np.where(y_test_original % 2 == 1, -1, 1)\n","\n","X_train, X_pool,t_train, t_pool, y_train, y_pool, ite_train, ite_pool = train_test_split(X_train,t_train, y_train, ite, test_size=0.9)\n","N_TRAIN = 1000\n","\n","# Create training data\n","px = np.where(y_train == 9, 0.5, 0.5/9)\n","six = np.random.choice(X_train.shape[0], size = N_TRAIN, p=px/np.sum(px))\n","\n","\n","ds = {\"X_training\": X_train[six,:],\n","      \"y_training\" : y_train[six],\n","      \"t_training\" : t_train[six],\n","      \"ite_training\" : ite_train[six],\n","      \"X_pool\": X_pool,\n","      \"t_pool\": t_pool,\n","      \"y_pool\": y_pool,\n","      \"ite_pool\": ite_pool,\n","      \"X_test\":X_test,\n","      \"y_test\":y_test,\n","      \"t_test\":t_test,\n","      \"ite_test\":ite_test}"],"metadata":{"id":"za1fvXYZSeB9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["asl = BaseActiveLearner(estimator = ASBETARNETIPM(two_model=False, conv=True),\n","                       acquisition_function=DDALIPM(no_query=10),\n","                       assignment_function=BaseAssignmentFunction(),\n","                       stopping_function = None,\n","                       no_query = 100,\n","                       dataset=ds)\n","asl.estimator.dataset = asl.dataset\n","asl.fit()"],"metadata":{"id":"TSD97iCj_0UF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_, unc_selection = asl.query(mode=\"uncertainty\", no_query=100)\n","# _, tc_selection  = asl.query(mode=\"treat_v_control\", no_query=10)\n","# _, tp_selection = asl.query(mode=\"train_v_pool\", no_query=10)\n","# _, all_selection = asl.query(mode=[\"uncertainty\", \"treat_v_control\", \"train_v_pool\"], no_query=10)\n","_, random_selection = asl.query(mode=\"random\", no_query=100)\n","_, rank_selection = asl.query(mode=\"accounting\", no_query=100)\n","\n","\n","#_, batch_selection = asl.query(mode=[\"batch\"], simulations=10)\n","print(y_pool[unc_selection])\n","print(y_pool[tc_selection])\n","print(y_pool[tp_selection])\n","print(y_pool[all_selection])\n","print(y_pool[rank_selection])"],"metadata":{"id":"hjc7s4rB5gY5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **CMF Micro data (main simulation)**  "],"metadata":{"id":"-AuTTs2aObfB"}},{"cell_type":"code","source":["def create_nonoverlap_data(data, N_train = 50, q=.4, sim_data=True, seed=42, col_to_use = 0):\n","  qs = np.quantile(data[\"X\"][:, col_to_use], q=q)\n","  if sim_data:\n","    x0ind = np.random.choice(np.where(data[\"X\"][:, col_to_use] < qs[0])[0],\n","                    size = int(N_train/2), replace=False)\n","    x1ind = np.random.choice(np.where(data[\"X\"][:, col_to_use] > qs[1])[0],\n","                    size = int(N_train/2), replace=False)\n","    X_train = data[\"X\"][[x0ind, x1ind], :].reshape((N_train, -1))\n","    t_train = np.array([np.zeros(int(N_train/2)), np.ones(int(N_train/2))]).ravel()\n","    y_train = np.array([data[\"y_0\"][x0ind], data[\"y_1\"][x1ind]]).ravel()\n","    tau_train = data[\"tau\"][np.array([x0ind, x1ind]).ravel()]\n","\n","  else:\n","    px0 = np.where(\n","        data[\"X\"][:, col_to_use] <= qs,\n","        1000,\n","        1\n","    )\n","    px1 = np.where(\n","        data[\"X\"][:, col_to_use] > qs,\n","        1000,\n","        1\n","    )\n","    # print(px0[(data[\"t\"]==1)] / np.sum( px0[(data[\"t\"]==1)]))\n","    x0ind = np.random.choice(np.arange(data[\"X\"].shape[0])[(data[\"t\"]==0)],\n","                             int(N_train/2),\n","                             replace=False,\n","                             p=px0[(data[\"t\"]==0)] / np.sum(px0[(data[\"t\"]==0)])\n","                             )\n","    x1ind = np.random.choice(np.arange(data[\"X\"].shape[0])[(data[\"t\"]==1)],\n","                            int(N_train/2),\n","                             replace=False,\n","                             p=px1[data[\"t\"]==1] / np.sum(px1[(data[\"t\"]==1)])\n","                             )\n","    ind_all = np.concatenate([x0ind, x1ind])\n","    X_train = data[\"X\"][ind_all,:]\n","    t_train = data[\"t\"][ind_all]\n","    y_train = data[\"y\"][ind_all]\n","    tau_train = np.zeros(y_train.shape[0])\n","\n","  X_pool = np.delete(data[\"X\"], [x0ind, x1ind], axis=0)\n","  t_pool =  np.delete(data[\"t\"], [x0ind, x1ind], axis=0)\n","  y_pool = np.delete(data[\"y\"], [x0ind, x1ind])\n","  y1_pool = np.delete(data[\"y_1\"], [x0ind, x1ind])\n","  y0_pool = np.delete(data[\"y_0\"], [x0ind, x1ind])\n","  tau_pool = np.delete(data[\"tau\"], [x0ind, x1ind])\n","  X_pool, X_test, t_pool, t_test, y_pool, y_test, y1_pool, y1_test, \\\n","  y0_pool, y0_test, tau_pool, tau_test  = train_test_split(X_pool,\n","  t_pool,\n","  y_pool,\n","  y1_pool,\n","  y0_pool,\n","  tau_pool,test_size=0.1, random_state = seed)\n","  ds = {\"X_training\" : X_train,\n","         \"y_training\" : y_train,\n","         \"t_training\" : t_train,\n","         \"ite_training\" : tau_train,\n","         \"X_pool\" : X_pool,\n","         \"t_pool\" : t_pool,\n","         \"y_pool\" : y_pool,\n","         \"ite_pool\": tau_pool,\n","         \"X_test\": X_test,\n","        \"y_test\" : y_test,\n","        \"t_test\" : t_test,\n","        \"ite_test\" : tau_test}\n","  return ds"],"metadata":{"id":"WxMJxRxtQTW-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_covariateshift_data(data, N_train = 50, q=.4, sim_data=True, seed=42, col_to_use=0):\n","  qs = np.quantile(data[\"X\"][:, col_to_use], q=q)\n","  if sim_data:\n","    xind = np.random.choice(np.arange(data[\"X\"].shape[0])[data[\"X\"][:, col_to_use] < qs[0]],\n","                    size = int(N_train), replace=False)\n","    x0ind = xind[:int(N_train/2)]\n","    x1ind = xind[int(N_train/2):]\n","    X_train = data[\"X\"][[x0ind, x1ind], :].reshape((N_train, -1))\n","    t_train = np.array([np.zeros(int(N_train/2)), np.ones(int(N_train/2))]).ravel()\n","    y_train = np.array([data[\"y_0\"][x0ind], data[\"y_1\"][x1ind]]).ravel()\n","    tau_train = data[\"tau\"][np.array([x0ind, x1ind]).ravel()]\n","  else:\n","    prob = np.where(data[\"X\"][:, col_to_use] < qs, 1000, 1)\n","    xind = np.random.choice(np.arange(data[\"X\"].shape[0]),\n","                    size = int(N_train), replace=False, p = prob/prob.sum())\n","    x0ind = xind[data[\"t\"][xind] == 0]\n","    x1ind = xind[data[\"t\"][xind] == 0]\n","    X_train = data[\"X\"][xind,:]\n","    t_train = data[\"t\"][xind]\n","    y_train = data[\"y\"][xind]\n","    tau_train = np.zeros(y_train.shape[0])\n","\n","  X_pool = np.delete(data[\"X\"], [x0ind, x1ind], axis=0)\n","  t_pool =  np.delete(data[\"t\"], [x0ind, x1ind], axis=0)\n","  y_pool = np.delete(data[\"y\"], [x0ind, x1ind])\n","  y1_pool = np.delete(data[\"y_1\"], [x0ind, x1ind])\n","  y0_pool = np.delete(data[\"y_0\"], [x0ind, x1ind])\n","  tau_pool = np.delete(data[\"tau\"], [x0ind, x1ind])\n","  X_pool, X_test, t_pool, t_test, y_pool, y_test, y1_pool, y1_test, \\\n","    y0_pool, y0_test, tau_pool, tau_test  = train_test_split(X_pool,\n","    t_pool,\n","    y_pool,\n","    y1_pool,\n","    y0_pool,\n","    tau_pool,test_size=0.1, random_state=seed)\n","  ds = {\"X_training\" : X_train,\n","         \"y_training\" : y_train,\n","         \"t_training\" : t_train,\n","         \"ite_training\" : tau_train,\n","         \"X_pool\" : X_pool,\n","         \"t_pool\" : t_pool,\n","         \"y_pool\" : y_pool,\n","         \"ite_pool\": tau_pool,\n","         \"X_test\": X_test,\n","        \"y_test\" : y_test,\n","        \"t_test\" : t_test,\n","        \"ite_test\" : tau_test}\n","  return ds"],"metadata":{"id":"DM6kaD_fnjJQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklift.datasets import fetch_criteo, fetch_hillstrom\n","from sklift.metrics import qini_auc_score\n","from sklift.viz import plot_qini_curve"],"metadata":{"id":"YK1muVY1OT6Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from causeinfer.data import hillstrom, cmf_micro"],"metadata":{"id":"WEZsoICtZ3k3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_raw = cmf_micro.load_cmf_micro(\n","    file_path=\"cmf_micro\", format_covariates=True, normalize=True\n",")\n","\n","df_full = pd.DataFrame(data_raw[\"dataset_full\"], columns=data_raw[\"dataset_full_names\"])"],"metadata":{"id":"2X4fGdFmaK4l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cmf_data = {\"X\":data_raw[\"features\"],\n","               \"y\": data_raw[\"response_biz_index\"],\n","               \"t\": data_raw[\"treatment\"],\n","               \"y_1\":np.zeros(data_raw[\"features\"].shape[0]),\n","               \"y_0\":np.zeros(data_raw[\"features\"].shape[0]),\n","               \"tau\":np.zeros(data_raw[\"features\"].shape[0])}\n","# cd = create_nonoverlap_data(criteo_data, N_train=5000, sim_data=False)"],"metadata":{"id":"IWHeZY5MP766"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ds = create_covariateshift_data(cmf_data, 500, q = .5, sim_data=False)\n","# asl = BaseActiveLearner(estimator = ASBETARNETIPM(two_model=False,\n","#                                                   binary_outcome=False,\n","#                                                   epochs=2,\n","#                                                   num_sim=5),\n","#                             acquisition_function=DDALIPM(no_query=100),\n","#                             assignment_function=BaseAssignmentFunction(),\n","#                             stopping_function = None,\n","#                             dataset=ds)\n","# asl.estimator.dataset = asl.dataset\n","# asl.fit()\n","# X_new, ix = asl.query(mode=\"accounting\", no_query=2, mmd = False)"],"metadata":{"id":"i6gI5BHJh8ha"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns"],"metadata":{"id":"2KmITDdogy0W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import matplotlib.pyplot as plt\n","CTU = 6\n","sns.set_style(\"whitegrid\")\n","qs = np.arange(0.0, 1.1, 0.1)\n","fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(10, 10))\n","for i, q in enumerate(qs):\n","    row, col = divmod(i, 3)\n","    ds = create_covariateshift_data(criteo_data, q=q,N_train=500,\n","                                sim_data=False, col_to_use = CTU)\n","    quan = np.quantile(criteo_data[\"X\"][:, CTU], q=[q])\n","    axes[row, col].hist(ds[\"X_training\"][ds[\"t_training\"] == 0, CTU], density=True, alpha=0.5, label=\"Control\")\n","    axes[row, col].hist(ds[\"X_training\"][ds[\"t_training\"] == 1, CTU], density=True, alpha=0.5, label=\"Treatment\")\n","    axes[row, col].axvline(x=quan, color=\"black\", linestyle=\"--\")\n","    axes[row, col].set_xlim(-2, 7)\n","    axes[row, col].legend()\n","    axes[row, col].set_title(f\"q = {q:.1f}\")\n","axes[3, 2].hist(ds[\"X_test\"][:, 0], density=True, alpha=0.5)\n","axes[3, 2].set_title(\"Density on test data\")\n","plt.tight_layout()\n","# plt.savefig(\"covshift_data.pdf\")\n","plt.show()"],"metadata":{"id":"5j9-Um_wN19S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pylift.eval import get_scores"],"metadata":{"id":"9AD-mVvqQMjT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sns.set_style('whitegrid')\n","# for i in range(1,10):\n","#   sns.kdeplot(create_nonoverlap_data(\n","#       criteo_data, 5000,col_to_use = 54, q = i/10, sim_data=False)[\"X_training\"][:, 54],\n","#             bw_method=0.3);"],"metadata":{"id":"W8nBL1VcP7-j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#NT = 50\n","#NQ = 20\n","#DISTORTION = \"covshift\"\n","for CTS in [6]:\n","# CTS = 54\n","  for DISTORTION in [\"overlap\"]:\n","    for LEVEL in [0.2, 0.5, 0.8]:\n","      for NT in [500]:\n","        for NQ in [500]:\n","          FNAME = f'drive/MyDrive/Colab Notebooks/data/{DISTORTION}_{LEVEL}_{NT}_{NQ}_ipm_cmf_col_{CTS}.pkl'\n","          try:\n","            with open(FNAME, 'rb') as f:\n","              res = pickle.load(f)\n","              if i in res.keys():\n","                continue\n","          except:\n","            res = {}\n","          # for i, dd in data.items():\n","          #   if i in res.keys():\n","          #     continue\n","          for i in range(10):\n","            print(f\"Running, n1 {NT}, n2 {NQ}, dist: {DISTORTION}, level {LEVEL}\")\n","            # if i in np.concatenate([np.arange(0, 325, 9), np.arange(3, 325, 9), np.arange(8, 325, 9)]).tolist():\n","            #   print(f\"DATA {i}\")\n","            if DISTORTION == \"overlap\":\n","              ds = create_nonoverlap_data(criteo_data,\n","                                          NT,\n","                                          q = LEVEL,\n","                                          sim_data=False, seed=i,\n","                                          col_to_use = CTS)\n","            elif DISTORTION == \"covshift\":\n","              ds = create_covariateshift_data(criteo_data,\n","                                              NT,\n","                                              q = LEVEL,\n","                                              sim_data=False,\n","                                              seed=i,\n","                                              col_to_use = CTS)\n","            asl = BaseActiveLearner(estimator = ASBETARNETIPM(two_model=False, binary_outcome=False,\n","                                                              epochs=100,\n","                                                              num_sim=100),\n","                                        acquisition_function=DDALIPM(no_query=NQ),\n","                                        assignment_function=BaseAssignmentFunction(),\n","                                        stopping_function = None,\n","                                        dataset=ds)\n","            asl.estimator.dataset = asl.dataset\n","            asl.fit()\n","            # asl.estimator.trainer.save_checkpoint(\"first_fit.ckpt\")\n","            data_to_restore = deepcopy(asl.dataset)\n","            res[i] = {}\n","            U = (1/ asl.dataset[\"t_training\"].shape[0])*asl.dataset[\"t_training\"].sum()\n","            ws = torch.from_numpy((data_to_restore[\"t_training\"]/(2*U) + (\n","                                    1-data_to_restore[\"t_training\"])/(2*(1-U))))\n","            preds = asl.predict(asl.dataset[\"X_test\"])\n","            sc_old = get_scores(asl.dataset[\"t_test\"],\n","                      asl.dataset[\"y_test\"],\n","                      preds.ravel(),\n","                      p=np.zeros(asl.dataset[\"y_test\"].shape[0]) + 0.51,\n","                        plot_type='aqini')[\"q1_aqini\"]\n","            # sc_old = qini_auc_score(asl.dataset[\"y_test\"],\n","            #                         preds.ravel(),\n","            #                         asl.dataset[\"t_test\"])\n","            #sc_old = asl.score()\n","            res[i].update({\"original\" : sc_old})\n","            print(f\"Selection score - original : {sc_old} \")\n","            pt = np.sum(asl.dataset[\"t_training\"] == 1) / asl.dataset[\"X_training\"].shape[0]\n","            for selection_mode in [\"accounting-F\",\n","                                    # \"uncertainty\",\n","                                    \"random\",\n","                                  #  \"batch\",\n","                                    # \"accounting-T\",\n","                                    ]:\n","                # Make query\n","                if selection_mode in [\"uncertainty\",\n","                                      \"treat_v_control-dist_to_labeled\",\n","                                      \"treat_v_control-dist_to_pool\",\n","                                      \"treat_v_control-dist_to_counter\",\n","                                      \"accounting-T\",\n","                                      \"accounting-F\"]:\n","                  if selection_mode.startswith(\"treat_v_control\"):\n","                    sel_mode, method = selection_mode.split(\"-\")\n","                    for method in [\"dist_to_labeled\", \"dist_to_pool\", \"dist_to_counter\"]:\n","                      X_new, ix = asl.query(mode=selection_mode, no_query=NQ, method = method)\n","                  elif selection_mode.startswith(\"accounting\"):\n","                    sel_mod, mmd_str = selection_mode.split(\"-\")\n","                    mmd_boolean = True if mmd_str == \"T\" else False\n","                    X_new, ix = asl.query(mode=sel_mod, no_query=NQ, mmd = mmd_boolean)\n","                    # X_acc = deepcopy(X_new)\n","                  else:\n","                    X_new, ix = asl.query(mode=selection_mode, no_query=NQ)\n","                elif selection_mode == \"batch\":\n","                  X_new, ix = asl.query(mode=selection_mode, no_query=NQ, simulations=50)\n","                elif selection_mode == \"all\":\n","                  X_new, ix = asl.query(mode=[\"uncertainty\", \"treat_v_control\", \"train_v_pool\"],no_query=NQ)\n","                elif selection_mode == \"random\":\n","                  X_new, ix = asl.query(acquisition_function=RandomAcquisitionFunction(no_query=NQ), no_query=NQ)\n","                  # X_r = deepcopy(X_new)\n","                asl.teach(ix)\n","                asl.estimator.dataset = asl.dataset\n","                asl.fit()\n","                preds = asl.predict(asl.dataset[\"X_test\"])\n","                sc_new = get_scores(asl.dataset[\"t_test\"],\n","                      asl.dataset[\"y_test\"],\n","                      preds.ravel(),\n","                      p=np.zeros(asl.dataset[\"y_test\"].shape[0]) + 0.51,\n","                        plot_type='aqini')[\"q1_aqini\"]\n","                res[i].update({selection_mode :sc_new})\n","                print(f\"Selection score - {selection_mode} : {sc_new} \")\n","              # Restore model and data\n","                asl.dataset = deepcopy(data_to_restore)\n","                asl.estimator.dataset = deepcopy(data_to_restore)\n","                with open(FNAME, 'wb') as f:\n","                  pickle.dump(res, f)"],"metadata":{"id":"_LHfGc34QfKs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ds = create_nonoverlap_data(criteo_data, 500,\n","                                        q = 0.1,\n","                                        sim_data=False, seed=i,\n","                                        col_to_use = 54)\n","asl = BaseActiveLearner(estimator = ASBETARNETIPM(two_model=False, binary_outcome=False,\n","                                                            epochs=100,\n","                                                            num_sim=100),\n","                                      acquisition_function=DDALIPM(no_query=500),\n","                                      assignment_function=BaseAssignmentFunction(),\n","                                      stopping_function = None,\n","                                      dataset=ds)\n","asl.estimator.dataset = asl.dataset\n","asl.fit()"],"metadata":{"id":"-5PcJKzdruI5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_new, ix = asl.query(mode=\"accounting\", no_query=500, mmd = False)"],"metadata":{"id":"qT5n70fesDro"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.set_style('whitegrid')\n","fig, ax =plt.subplots(4,1,sharex=True, sharey=True)\n","for m in [\"training\", \"ddal\", \"random\", \"test\"]:\n","  if m == \"training\":\n","    sns.kdeplot(asl.dataset[\"X_training\"][:, 54][asl.dataset[\"t_training\"] == 1],\n","                bw_method=0.3, label=\"training data\", ax=ax[0]);\n","    sns.kdeplot(asl.dataset[\"X_training\"][:, 54][asl.dataset[\"t_training\"] == 0],\n","                bw_method=0.3, label=\"training data\", ax=ax[0]);\n","  elif m == \"ddal\":\n","    sns.kdeplot(X_new[:, 54][asl.dataset[\"t_pool\"][ix] == 1],\n","                bw_method=0.3, label=\"DDAL\", ax=ax[1]);\n","    sns.kdeplot(X_new[:, 54][asl.dataset[\"t_pool\"][ix] == 0],\n","                bw_method=0.3, label=\"DDAL\", ax=ax[1]);\n","  elif m == \"random\":\n","    randix = np.random.randint(0, asl.dataset[\"X_pool\"].shape[0], 500)\n","    sns.kdeplot(asl.dataset[\"X_pool\"][:, 54][randix][asl.dataset[\"t_pool\"][randix] == 1],\n","                bw_method=0.3, label=\"random\", ax=ax[2]);\n","    sns.kdeplot(asl.dataset[\"X_pool\"][:, 54][randix][asl.dataset[\"t_pool\"][randix] == 0],\n","                bw_method=0.3, label=\"random\", ax=ax[2]);\n","  else:\n","    sns.kdeplot(asl.dataset[\"X_test\"][:, 54][asl.dataset[\"t_test\"] == 1],\n","                bw_method=0.3, label=\"random\", ax=ax[3]);\n","    sns.kdeplot(asl.dataset[\"X_test\"][:, 54][asl.dataset[\"t_test\"] == 0],\n","                bw_method=0.3, label=\"random\", ax=ax[3]);\n","plt.tight_layout()"],"metadata":{"id":"qJ0_cJQLTR2R"},"execution_count":null,"outputs":[]}]}